\chapter{Practical Part}\label{ch:Practical}
\section{Papers}
	\subsection{Introduction}
		This project aims to provide users with an agent that operates entirely offline, without performing any searches or retrieving information from the internet. The core objective is to ensure that every response the agent generates is grounded in trustworthy, scientifically validated knowledge. To achieve this, the project focuses on building a carefully curated database composed solely of verified scientific information.
		
		The most reliable way to guarantee the accuracy and credibility of the agent’s knowledge base is to manually compile and review scientific articles. By selecting only high-quality studies and extracting the essential findings, the project ensures that the agent embeds information that has already been thoroughly vetted. This approach prevents the introduction of unverified, misleading, or low-quality data and allows the agent to operate with a high degree of scientific integrity. As a result, users can interact with the system confidently, knowing that its responses are based solely on pre-approved, evidence-based sources.
	
	\subsection{Research}
		 The goal is to provide users with guidance supported by solid research. To do it an extensive review was conducted focusing on studies related to color, visual and auditory frequencies, and their effects on the human brain. After several hours of analysis, approximately 60–70 papers were selected for each topic, prioritizing those that offered the most reliable and impactful insights for the tool’s development. 
		\\
		Once the papers had been selected, the next step was to construct the database. To organize the information systematically, a CSV file was created to store and classify the key details from each article. The following structure, shown in Fidure \ref{fig:ExampleOfCSV} was chosen:
		\begin{itemize}
			\item \textbf{Title \textsubscript}
			
			\item \textbf{Author \textsubscript}
			
			\item \textbf{Year \textsubscript}
			
			\item \textbf{Sumary: \textsubscript}
				This section contains a condensed version of the paper’s abstract. Because abstracts are typically accessible for free, they provide enough information for the system to understand the essence of each study without requiring full-text access.
			
			\item \textbf{Keywords: \textsubscript}
			Keywords were included to enable the system to retrieve the most relevant papers based on the prompts provided by the user. When embeddings are generated, these keywords help the system match user queries with the scientific articles that best fit the topic, ensuring accurate and efficient information retrieval.
			
		\end{itemize}
		
		
		This structured approach ensures that the agent can navigate the database effectively and rely on well-organized, high-quality scientific data.
		
		\begin{figure} [H]
			\centering
			\includegraphics[width=1\linewidth]{gfx/ExampleOfCSV.png}
			\caption{Example of a CSV with the Structure of the project}
			\label{fig:ExampleOfCSV}
		\end{figure}
		
		The next step involved processing the information contained in the CSV file. To ensure that the agent could generate embeddings effectively, it was necessary to remove any characters that might interfere with processing, such as accents, apostrophes, and other special symbols. These characters can cause inconsistencies or errors during embedding generation, so cleaning the data was essential.
		
		To accomplish this, a script was created to automatically convert and sanitize the CSV files, ensuring that all entries were standardized and compatible with the agent’s requirements. This script, titled Papers\_clean.py, performs the necessary preprocessing steps and prepares the dataset for seamless integration. The full code for this script is included in the appendix \ref{sec: Clean_papers}.
	