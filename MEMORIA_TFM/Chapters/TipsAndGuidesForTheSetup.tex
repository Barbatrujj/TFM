\chapter{Tips and Guides for the Setup}
\label{ch:tips_guides_setup}

\section{How to connect an agent with memory}
\label{sec: Connect_agent_mwith_memory}
	To interact with an agent, you typically use the addMessage operator, which sends a message to the agent and receives a response as seen in Figure \ref{fig: Agent1}. This works well for single exchanges: you ask something, the agent replies, and the interaction ends there. However, addMessage does not support memory, so it cannot maintain an ongoing conversation. Each new message is treated as an isolated request.
	
	If you want to create a continuous dialogue with an agent, one in which the agent remembers previous messages, you should use the agentSession operator instead. As shown in Figure \ref{fig: Agent3}, this operator allows you to establish a session that preserves conversational context. To use it, open the operator, go to the Agent Session section, and attach the agent you want to interact with. Once connected, the agent will be able to maintain memory throughout the session. 
	
	Additionally, you can enhance the experience by connecting a chatViewer element to the agentSession. This provides a more intuitive, user-friendly interface for viewing and conducting the conversation.	

	\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/Agent1.jpeg}
		\caption{Connection between the operator addMessage and an Agent in Touch Designer} 
		\label{fig: Agent1}
	\end{figure}
	
	\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/Agent3.jpeg}
		\caption{Connection between the operator agentSession and an Agent in Touch Designer} 
		\label{fig: Agent3}
	\end{figure}
	
\section{How to introduce your information docs to the agent}
\label{sec:How_introduce_own_information_agent}

	The agents within LOPs have a limited ability to operate independently, but the real strength of the system lies in its capacity to train an LLM model using custom documents. To achieve this, the following steps must be followed, as shown in the figure \ref{fig: SourceDocs}:
	
	\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/SourceDocs.jpeg}
		\caption{Workflow for the connection of personal documentation to make the agent learn based in the LLM} 
		\label{fig: SourceDocs}
	\end{figure}
	
	
	
	\begin{itemize}
		\item{\textbf{Create a source\_docs operator}}: This operator can receive files in PDF, TXT, Markdown, Notes, or code formats. Essentially, it serves as the raw knowledge base from which the agent will learn.
		
		\item{\textbf{Add a rag\_index operator}}: 	LLM models cannot directly search through text; instead, they rely on metadata to retrieve information. The rag\_index processes the provided documents by splitting the text into chunks, generating embeddings for each chunk, and constructing a vector database. This database allows the system to efficiently access the relevant portions of the documentation. The operator then stores the metadata associated with these embeddings.
		
		\item{\textbf{Use a RAG retriever operator}}:This component is responsible for receiving the agent’s query and returning the most relevant information. It takes in the agent’s question, instructions, and conversation history, and from this input, it retrieves the 3–5 most relevant text fragments from the documents provided via source\_docs. These fragments are then automatically injected into the agent’s prompt, enabling it to respond accurately and contextually.
		
	\end{itemize}
	
	This workflow ensures that the agent can use custom knowledge effectively, grounding its responses in the documentation provided during training. As seen in figure \ref{fig: AgentConnections}, all these external tools have to be connected to the agent.
	
	\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/ConnectionsOfTheAgent.jpeg}
		\caption{External tools connected to an agent} 
		\label{fig: AgentConnections}
	\end{figure}

\section{Problem with RAG Index chunks}
\label{sec:problemwithRagIndexChunks}
	One of the problems encountered during the assembly of this project has been the size of the chunks used by the LOPS RAG index.  The previous section \ref{sec:How_introduce_own_information_agent} discussed how to make an agent learn from its own documents. In this case, the problem occurs within the Rag\_index operator. Once the Source\_docs has been configured, Rag\_index is connected to it. This operator has the interface shown in the figure \ref{fig: Rag_index_setup}. When you click the “create index” button, a relatively common error may occur. The indexing process begins normally, but after a few moments, an error message suddenly appears on the screen, as shown in the figure \ref{fig: Rag_Index_error} .
	
	\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/RagIndexsetup.png}
		\caption{Parameters of the rag\_index operator} 
		\label{fig: Rag_index_setup}
	\end{figure}
	
	
		\begin{figure} [H]
		\centering
		\includegraphics[width=1\linewidth]{gfx/EmbeddingError1.jpeg}
		\caption{Error creating index in rag\_index} 
		\label{fig: Rag_Index_error}
	\end{figure}
	
	
	This error message suggests a problem related to TCP ports, but in reality, it is caused by chunks that are too large for the system to process. Chunks are units of information divided into manageable fragments, commonly used in computing to handle large files efficiently. They play a key role in how data is organized, segmented, and processed. In this case, oversized chunks can overload the system, causing the indexing process to fail.
	
	\vspace{0.5cm}
	
	There are two possible solutions to address this issue:
	
	\begin{itemize}
		\item{\textbf{Reduce the chunk size}}: This is the simplest approach, although it does not guarantee success in every case. In the chunk\_size field, you can decrease the size of each chunk so that the system can handle them more easily. A practical starting value is 256, which often resolves the issue by making the chunks small enough for the indexer to process without errors.
		
		\item{\textbf{Modify the IndexCreatorEXT operator inside the Rag\_index operator}}: This approach provides more control over the indexing process. As seen in Figure \ref{fig: Rag_index_structure}, rag\_index operator begins with the documents\_table node, which stores the raw documents received by the system. This data then passes to stats\_table and index\_info\_table, which record metadata such as the number of chunks, their average size, and the index ID. The in1 node receives the documents coming from the external source\_docs, while IndexCreatorEXT oversees the entire indexing operation by calling Ollama to generate the embeddings. After that, a switch operator determines which view should be displayed, and finally, disp outputs the result of the processed index.
		
		\begin{figure} [H]
			\centering
			\includegraphics[width=1\linewidth]{gfx/StructureIndexCreatorTEX.png}
			\caption{Structure inside of the Rag\_index.} 
			\label{fig: Rag_index_structure}
		\end{figure}
		
		To fix this error, you need to access the IndexCreatorEXT operator. Inside it, you will find a block of Python code that controls how the embeddings are generated. The solution requires modifying this code directly.
		
		Navigate to line 608, and, as shown in the figure \ref{fig: Code_index_creator}, add the following lines:
		
		\vspace{0.5cm}
		
		\textit{from llama\_index.core.embeddings import BaseEmbedding
			\\from llama\_index.core import Settings	
			\\Settings.embed\_batch\_size = 1
			\\Settings.chunk\_size = 256}
	
		
		
		\begin{figure} [H]
			\centering
			\includegraphics[width=1\linewidth]{gfx/CodeIndexCreator.png}
			\caption{Lines added to fix the error.} 
			\label{fig: Code_index_creator}
		\end{figure}
		These settings reduce the number of elements the computer processes at once by lowering both the batch size and the chunk size. As a result, the system becomes much less likely to overload during indexing, helping prevent the error and allowing the RAG index to be created successfully.
		
	\end{itemize}
	

	